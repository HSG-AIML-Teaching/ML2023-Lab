{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yaWalgW_d7j"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"figures/banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dojtwAh1Ww1B"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"figures/hsg_logo.png\">\n",
    "\n",
    "##  Lab 03 - \"Introduction to Pytorch\"\n",
    "\n",
    "Machine Learning, University of St. Gallen, Spring Term 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    " - Become familiar with PyTorch\n",
    " - Understand the basics of automatic differentiation\n",
    " - Implement a simple linear model with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Use Deep Learning Libraries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a powerful library for numerical computing and can be used to build and train neural networks from scratch. However, there are two significant limitations to using pure Numpy for deep learning:\n",
    "\n",
    "1. Numpy does not provide built-in support for GPU acceleration.\n",
    "2. Numpy does not offer automatic differentiation out of the box.\n",
    "\n",
    "As a result, using Numpy for deep learning is not always the most efficient or practical option. Instead, frameworks like PyTorch have been designed to address these limitations and provide developers with powerful tools for building, training, and deploying deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Which Library to Use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the major libraries that we can use for deep learning are: PyTorch, Jax, TensorFlow and Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"figures/dl_libraries.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare several features in these libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature        | PyTorch           | JAX               | TensorFlow 2.0    | Chainer           |\n",
    "|----------------|-------------------|-------------------|-------------------|-------------------|\n",
    "| Main Focus     | Deep Learning     | Numerical Computing| Deep Learning     | Deep Learning     |\n",
    "| Backends | PyTorch | XLA, NumPy | TensorFlow | NumPy |\n",
    "| Programming Language Support | Python | Python, Julia | Python | Python |\n",
    "| GPU Support | Yes         | Yes         | Yes         | Yes         |\n",
    "| Computation Graph | Dynamic           | Dynamic            | Dynamic (since version 2.0)            | Dynamic            |\n",
    "| Auto Differentiation | Yes             | Yes               | Yes              | Yes               |\n",
    "| APIs for Model Building | Torch.nn | Flax, Haiku | Keras, Estimators, Layers | Chainer.links, Chainer.functions |\n",
    "| Distributed Training | Yes             | Yes                | Yes              | Yes                |\n",
    "| Visualization   | TensorBoard       | No                | TensorBoard       | No                |\n",
    "| Community       | Large, Active  | Small, Active   | Large, Active  | Small, Active   |\n",
    "| Platform Support | Windows, Linux, macOS | Linux, macOS | Windows, Linux, macOS | Windows, Linux, macOS |\n",
    "| Ease of Debugging | Good | Good | Okay | Okay |\n",
    "| Mobile Deployment | PyTorch Mobile | No | TensorFlow Lite | ChainerX |\n",
    "| Primary Developer| Meta AI (Gov. by Linux Foundations) |Google | Google |  Preferred Networks|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch for our tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An Overview of the PyTorch Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"figures/pytorch_packages.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `autograd`: This package is used for automatic differentiation. The autograd package is essential for training neural networks using backpropagation, as it allows users to easily compute gradients of the loss function with respect to the model parameters.\n",
    "\n",
    "+ `nn`: This package provides a high-level API for building neural networks in PyTorch. It includes the most common types of layers such as convolutional layers, pooling layers, and linear layers, as well as activation functions and loss functions. The `nn` module also provides tools for building custom layers and models using PyTorch tensors.\n",
    "\n",
    "+ `optim`: This package provides various optimization algorithms for training neural networks in PyTorch. It includes popular optimization methods such as Stochastic Gradient Descent (SGD), Adam, and Adagrad. The optim module also provides tools for customizing the learning rate and weight decay, as well as implementing learning rate schedulers.\n",
    "\n",
    "+ `utils`: This package provides a variety of utility functions such as data loading and and visualization. For example, the `torch.utils.data` module contains classes and functions for loading and preprocessing data, and the `torch.utils.tensorboard` module provides support for visualizing training and validation metrics in via `TensorBoard`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computational Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 What are computational graphs and why do we need them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational graph in a directed acyclic graph (DAG) that represents the flow of information through the network. It consists of nodes that represent mathematical operations and edges that represent the flow of data between the nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a very simple function:\n",
    "\n",
    "$$f(x) = w \\times x + b$$\n",
    "\n",
    "Here $x$ is the input and $w$ and $b$ are (learnable) parameters. We want to change $w$ and $b$ such that the output of the function gets as close as possible to a target output (ground-truth).\n",
    "\n",
    "Let's say the output of $f$ for $x=0.4$ is equal to $0.7$, but we want to to be equal to $1.0$:\n",
    "\n",
    "$$f(0.4) = 0.7 \\rightarrow f(0.4) = 1.0$$\n",
    "\n",
    "To do so, we first mesaure the difference between the desired output and the actual output of the function and we call it the loss ($l$):\n",
    "\n",
    "$$l = ||f(0.4) - 1.0||_{2}^{2}$$\n",
    "\n",
    "Then, to estimate the amount of required change in $w$ and $b$ to get closer to the desired value, we need to compute the gradients of the loss w.r.t. the functions parameters:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial w}, \\frac{\\partial l}{\\partial b}$$\n",
    "\n",
    "And finally update $w$ an $b$ using gradient descent:\n",
    "$$w_{new} \\leftarrow w - \\alpha  \\frac{\\partial l}{\\partial w}$$\n",
    "$$b_{new} \\leftarrow b - \\alpha \\frac{\\partial l}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can do the same steps in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.4)  # input tensor\n",
    "y = torch.tensor(1.0)  # expected output\n",
    "w = torch.tensor(0.2, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "b = torch.tensor(0.0, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "z = w * x + b\n",
    "loss = torch.norm(z - y, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the tensor operations above creates the following computational graphs that enables automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px\" src=\"figures/comp-graph.png\">\n",
    "<sup> Image adapted from: <a href=\"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</a> <sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, the computational graph of the function above is created dynamically or on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the loss is computed and the computational graphs is formed (in the background), we can compute the gradients for the learnable parmeters. But first let's check what are the gradient values for the (learnable) parameters $w$ and $b$ before computing the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to compute all gradients in a computational graphs is to call `.backward()` on the loss terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the gradients again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4000)\n",
      "tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voila! the gradients are there. Remember that after calling `.backward()` the computational graph is removed for computational reasons. For most application you don't need to keep the computational graph, but there are ways to keep it which is outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Another way to compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute partial derrivatives w.r.t. particular parameters in the model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.4)  # input tensor\n",
    "y = torch.tensor(1.0)  # expected output\n",
    "w = torch.tensor(0.2, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "b = torch.tensor(0.0, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "z = x * w + b\n",
    "loss = torch.norm(z - y, p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4000), tensor(-1.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Neural Network with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement more advanced or complex neural networks, we need to use the `nn` package. Let's start with a simple example, a linear (or affine) mapping:\n",
    "\n",
    "A linear mapping, takes a vector of size $n$ as input and outputs a vector of size $m$.\n",
    "\n",
    "$$v = A*x + b$$\n",
    "\n",
    "Here $A$ is a matrix of shape $m \\times n$, $x$ has shape $1 \\times n$, and $v$ has shape $1 \\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `Linear` modules from `torch.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(in_features=5, out_features=3, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed it a random tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5352, 0.7310, 0.9065, 0.6696, 0.0924]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply linear operator to the input $x$, we use the \"call\" operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4710, -0.4611,  0.9950]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "v = linear(x)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Linear`, takes care of all learnable parameters ($A$ and $b$). But where are the parameters in `linear`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.3720,  0.1446, -0.0397,  0.0428, -0.3326],\n",
      "        [-0.0732, -0.0224, -0.1021, -0.4232,  0.0306],\n",
      "        [ 0.2160,  0.0673,  0.3663,  0.3307,  0.2147]], requires_grad=True)\n",
      "bias : Parameter containing:\n",
      "tensor([ 0.2042, -0.0325,  0.2569], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for n, p in linear.named_parameters():\n",
    "    print(n, \":\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Implementing Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement each layer/transformation separately, but for more complex models we need a better way. Let's assume we want to design a model with three consecutive linear transformations for given input $x$:\n",
    "\n",
    "$v = A_3 (A_2 (A_1 x + b_1) + b_2) + b_3$\n",
    "\n",
    "$v_1 = A_1 x + b_1, \\rightarrow v_2 = A_2 v_1 + b_2, \\rightarrow v_3 = A_3 v_2 + b_3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement our own neural network as a sequence of operations applied to an input tensor. Therefore, we need to define a class that inherits from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel(nn.Module):    \n",
    "    \n",
    "    # Class constructor: called when we create an instance of the model\n",
    "    def __init__(self):\n",
    "        # Call super class constructor: to initialize default configs in PyTorch `nn` modules\n",
    "        super(MyLinearModel, self).__init__()\n",
    "        \n",
    "        # First linear layer (transformation)\n",
    "        self.linear_1 = nn.Linear(10, 20, bias=True) \n",
    "        \n",
    "        # Second linear layer\n",
    "        self.linear_2 = nn.Linear(20, 5, bias=True)\n",
    "        \n",
    "        # Third linear layer\n",
    "        self.linear_3 = nn.Linear(5, 2, bias=True) \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the first linear transformation\n",
    "        x = self.linear_1(x)\n",
    "        \n",
    "        # Apply the second linear transformation\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        # Apply the third linear transformation\n",
    "        x = self.linear_3(x)\n",
    "        \n",
    "        # Return the final tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyLinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "v = my_model.forward(x) # or just my_model(x)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizing a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.optim` package provides a collection of optimization algorithms and tools that can be used to train neural networks. We don't have to use these algorithms and instead directly update the models, but using optimizers can speed-up implementation and avoid potential errors in the code. Some of the most popular optimizers are: \n",
    "\n",
    "- Stochastic Gradient Descent: `SGD`\n",
    "- Adam: `Adam`\n",
    "- RMSProp: `RMSProp`\n",
    "- AdaGrad: `AdaGrad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Without `torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, forwrad the input to the neural network and compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1297, 0.3455]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "my_model = MyLinearModel()\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Inputs and targets\n",
    "x = torch.randn(1, 10)\n",
    "target = torch.ones(1, 2)\n",
    "\n",
    "print(my_model(x))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagate and compute gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's prediction\n",
    "pred = my_model(x)\n",
    "\n",
    "# Loss\n",
    "loss = torch.mean(pred - target)\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to manually use the gradients to update model's parameters. Before that, let's check the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in my_model.parameters():\n",
    "#     print(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update weights in each parameter as: $w^{(new)} = w - \\alpha \\frac{\\partial L}{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in my_model.parameters():\n",
    "    p.data = p.data - alpha * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 With `torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch optimizers are algorithms that update the parameters of a model in every update step. To use them, we only need to assign the parameters that we want to be update when creating an instance of the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyLinearModel()\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "# Define optimizer over model's paramters\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=alpha)\n",
    "\n",
    "# Inputs and targets\n",
    "x = torch.randn(1, 10)\n",
    "target = torch.ones(1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, similar prediction, loss computation and backpropagation steps as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's prediction\n",
    "pred = my_model.forward(x)\n",
    "\n",
    "# Loss\n",
    "loss = torch.mean(pred - target)\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform optimizer update. The optimizer takes care of updating the parameters in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two major parts of data loading are:\n",
    "\n",
    "- Dataset handling\n",
    "- Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `torchvision` contains many useful computer vision utilities and datasets. We can easily download the datasets and directly use them. For example the dataset MNIST can be dataloaded via `torchvision.datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"./data\", download=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `torchvision` classes inherit from class `Dataset`. This means that you can directly access individual samples in data dataset via an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x7FEEF04705E0>, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification dataset, each index returns a tuple, and image and a label. \n",
    "\n",
    "In the `MNIST` dataset, it returns the PIL image directly, let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[520][0])\n",
    "print(\"label: \", dataset[520][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's cool. But we can't feed raw images directly to a neural network!\n",
    "\n",
    "We need to use `transformations` to transform every samples of dataset when we try to access them. Let's create the same MNIST dataset, now with transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"./data\", download=True, train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to use a dataset that doesn't exist in `tochvision.datasets`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, if we have a datasaet with many samples, we need to find a way to load the data in batches. One easy way is to use `torch.data.utils.DataLoader`. Let's assume we have a dataset, we can create a dataloader to randomly load data for us and return it as batches of arbitrary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "tensor([1, 4, 9, 6, 7, 1, 8, 5, 5, 8, 8, 3, 2, 3, 4, 1, 4, 0, 2, 3, 4, 3, 1, 6,\n",
      "        4, 2, 2, 8, 6, 7, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aut1dJXmWw1O",
    "Ks081EJEWw1P",
    "vyqnqndjWw1S",
    "ucTxc7GGWw1c",
    "hJhKTaHnWw1i",
    "8nyWq1X-Ww1n",
    "e1l8HbUzWw1v"
   ],
   "name": "lab_04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
