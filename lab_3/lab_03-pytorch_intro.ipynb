{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yaWalgW_d7j"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"figures/banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dojtwAh1Ww1B"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"figures/hsg_logo.png\">\n",
    "\n",
    "##  Lab 03 - \"Introduction to Pytorch\"\n",
    "\n",
    "Machine Learning, University of St. Gallen, Spring Term 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    " - Become familiar with PyTorch\n",
    " - Understand the basics of automatic differentiation\n",
    " - Implement a simple linear model with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use Deep Learning Libraries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a powerful library for numerical computing and can be used to build and train neural networks from scratch. However, there are two significant limitations to using pure Numpy for deep learning:\n",
    "\n",
    "1. Numpy does not provide built-in support for GPU acceleration.\n",
    "2. Numpy does not offer automatic differentiation out of the box.\n",
    "\n",
    "As a result, using Numpy for deep learning is not always the most efficient or practical option. Instead, frameworks like PyTorch have been designed to address these limitations and provide developers with powerful tools for building, training, and deploying deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Which Library to Use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the major libraries that we can use for deep learning are: PyTorch, Jax, TensorFlow and Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"figures/dl_libraries.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare several features in these libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature        | PyTorch           | JAX               | TensorFlow 2.0    | Chainer           |\n",
    "|----------------|-------------------|-------------------|-------------------|-------------------|\n",
    "| Main Focus     | Deep Learning     | Numerical Computing| Deep Learning     | Deep Learning     |\n",
    "| Backends | PyTorch | XLA, NumPy | TensorFlow | NumPy |\n",
    "| Programming Language Support | Python | Python, Julia | Python | Python |\n",
    "| GPU Support | Yes         | Yes         | Yes         | Yes         |\n",
    "| Computation Graph | Dynamic           | Dynamic            | Dynamic (since version 2.0)            | Dynamic            |\n",
    "| Auto Differentiation | Yes             | Yes               | Yes              | Yes               |\n",
    "| APIs for Model Building | Torch.nn | Flax, Haiku | Keras, Estimators, Layers | Chainer.links, Chainer.functions |\n",
    "| Distributed Training | Yes             | Yes                | Yes              | Yes                |\n",
    "| Visualization   | TensorBoard       | No                | TensorBoard       | No                |\n",
    "| Community       | Large, Active  | Small, Active   | Large, Active  | Small, Active   |\n",
    "| Platform Support | Windows, Linux, macOS | Linux, macOS | Windows, Linux, macOS | Windows, Linux, macOS |\n",
    "| Ease of Debugging | Good | Good | Okay | Okay |\n",
    "| Mobile Deployment | PyTorch Mobile | No | TensorFlow Lite | ChainerX |\n",
    "| Primary Developer| Meta AI (Gov. by Linux Foundations) |Google | Google |  Preferred Networks|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PyTorch for our tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Overview of the PyTorch Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px\" src=\"figures/pytorch_packages.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `autograd`: This package is used for automatic differentiation. The autograd package is essential for training neural networks using backpropagation, as it allows users to easily compute gradients of the loss function with respect to the model parameters.\n",
    "\n",
    "+ `nn`: This package provides a high-level API for building neural networks in PyTorch. It includes the most common types of layers such as convolutional layers, pooling layers, and linear layers, as well as activation functions and loss functions. The `nn` module also provides tools for building custom layers and models using PyTorch tensors.\n",
    "\n",
    "+ `data`: This package provides tools for loading and processing data in PyTorch. It includes data loaders, which can be used to load and transform data from various sources, such as files or databases. The data module also includes utilities for data augmentation and sampling.\n",
    "\n",
    "+ `optim`: This sub-package provides various optimization algorithms for training neural networks in PyTorch. It includes popular optimization methods such as Stochastic Gradient Descent (SGD), Adam, and Adagrad. The optim module also provides tools for customizing the learning rate and weight decay, as well as implementing learning rate schedulers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are computational graphs and why do we need them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational graph in a directed acyclic graph (DAG) that represents the flow of information through the network. It consists of nodes that represent mathematical operations and edges that represent the flow of data between the nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a very simple function:\n",
    "\n",
    "$$f(x) = x \\times w + b$$\n",
    "\n",
    "Here $x$ is the input and $w$ and $b$ are (learnable) parameters. We want to change $w$ and $b$ such that the output of the function gets as close as possible to a target output (ground-truth).\n",
    "\n",
    "Let's say the output of $f$ for $x=0.4$ is equal to $0.7$, but we want to to be equal to $1.0$:\n",
    "\n",
    "$$f(0.4) = 0.7 \\rightarrow f(0.4) = 1.0$$\n",
    "\n",
    "To do so, we first mesaure the difference between the desired output and the actual output of the function and we call it the loss ($l$):\n",
    "\n",
    "$$l = ||f(0.4) - 0.7||_{2}^{2}$$\n",
    "\n",
    "Then, to estimate the amount of required change in $w$ and $b$ to get closer to the desired value, we need to compute the gradients of the loss w.r.t. the functions parameters:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial w}, \\frac{\\partial l}{\\partial b}$$\n",
    "\n",
    "And finally update $w$ an $b$ using gradient descent:\n",
    "$$w_{new} \\leftarrow w - \\alpha  \\frac{\\partial l}{\\partial w}$$\n",
    "$$b_{new} \\leftarrow b - \\alpha \\frac{\\partial l}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can do the same steps in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.4)  # input tensor\n",
    "y = torch.tensor(1.0)  # expected output\n",
    "w = torch.tensor(0.2, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "b = torch.tensor(0.0, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "z = x * w + b\n",
    "loss = torch.norm(z - y, p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the tensor operations above creates the following computational graphs that enables automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px\" src=\"figures/comp-graph.png\">\n",
    "<sup> Image adapted from: <a href=\"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</a> <sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the loss is computed and the computational graphs is formed (in the background), we can compute the gradients for the learnable parmeters. But first let's check what are the gradient values for the learnable parameters $w$ and $b$ before computing the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to compute all gradients in a computational graphs is to call `.backward()` on the loss terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the gradients again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4000)\n",
      "tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voila! the gradients are there. Remember that after calling `.backward()` the computational graph is removed for computational reasons. For most application you don't need to keep the computational graph, but there are ways to keep it which is outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to compute gradients *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute partial derrivatives w.r.t. particular parameters in the model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.4)  # input tensor\n",
    "y = torch.tensor(1.0)  # expected output\n",
    "w = torch.tensor(0.2, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "b = torch.tensor(0.0, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "z = x * w + b\n",
    "loss = torch.norm(z - y, p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4000), tensor(-1.))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network with `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement more advanced or complex neural networks, we need to use the `nn` package. Let's start by a simple example, a linear (or affine) mapping:\n",
    "\n",
    "A linear mapping, takes a vector of size $n$ as input and outputs a vector of size $m$.\n",
    "\n",
    "$$v = A*x + b$$\n",
    "\n",
    "Here $A$ is a matrix of shape $m \\times n$. $x$ has shape $1 \\times n$, $v$ has shape $1 \\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `Linear` modules from `torch.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(in_features=5, out_features=3, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed it a random tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1065, 0.0988, 0.0673, 0.1315, 0.2903]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply linear operator to the input $x$, we use the \"call\" operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2129,  0.2258, -0.5205]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "v = linear(x)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Linear`, takes care of all learnable parameters ($A$ and $b$). But where are the parameters in `linear`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.2935,  0.1808, -0.2188,  0.3289,  0.1805],\n",
      "        [-0.1033,  0.0085,  0.3332, -0.0218, -0.0499],\n",
      "        [-0.2402,  0.0237, -0.0532, -0.3007, -0.0706]], requires_grad=True)\n",
      "bias : Parameter containing:\n",
      "tensor([ 0.0829,  0.2309, -0.4337], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for n, p in linear.named_parameters():\n",
    "    print(n, \":\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement each layer/transformation separately, but for more complex models we need a better way. Let's assume we want to design a model with three consecutive linear transformations for given input $x$:\n",
    "\n",
    "$v = A_3 (A_2 (A_1 x + b_1) + b_2) + b_3$\n",
    "\n",
    "$v_1 = A_1 x + b_1, \\rightarrow v_2 = A_2 v_1 + b_2, \\rightarrow v_3 = A_3 v_2 + b_3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement our own neural network as a sequence of operations applied to an input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel(nn.Module):    \n",
    "    \n",
    "    # Class constructor: called when we create an instance of the model\n",
    "    def __init__(self):\n",
    "        # Call super class constructor: to initialize default settings in PyTorch `nn` modules\n",
    "        super(MyLinearModel, self).__init__()\n",
    "        \n",
    "        # First linear layer (transformation)\n",
    "        self.linear_1 = nn.Linear(10, 20, bias=True) \n",
    "        \n",
    "        # Second linear layer\n",
    "        self.linear_2 = nn.Linear(20, 5, bias=True)\n",
    "        \n",
    "        # Third linear layer\n",
    "        self.linear_3 = nn.Linear(5, 2, bias=True) \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the first linear transformation\n",
    "        x = self.linear_1(x)\n",
    "        \n",
    "        # Apply the second linear transformation\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        # Apply the third linear transformation\n",
    "        x = self.linear_3(x)\n",
    "        \n",
    "        # Return the final tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyLinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "v = my_model(x)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aut1dJXmWw1O",
    "Ks081EJEWw1P",
    "vyqnqndjWw1S",
    "ucTxc7GGWw1c",
    "hJhKTaHnWw1i",
    "8nyWq1X-Ww1n",
    "e1l8HbUzWw1v"
   ],
   "name": "lab_04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
